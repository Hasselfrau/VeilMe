{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VeilMeFinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Bj8rG2WQM1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "43beee51-dd95-4a3a-dc39-1da0241d84e4"
      },
      "source": [
        "! sudo pip --no-cache-dir install face_recognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=3e48cab14f25afc2360392e60d4714b9c9eb399cf12d7fbb6535be98d89a7f1e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dymaoxi3/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR7kFKHNQbf8"
      },
      "source": [
        "import dlib\n",
        "import imutils\n",
        "import cv2\n",
        "import face_recognition\n",
        "import skimage.io\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 8)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbVTRdeLQyrb"
      },
      "source": [
        "def compare_photos(img1, img2, error_message='BadInput'):\n",
        "  '''\n",
        "  Takes: \n",
        "    two files in format 'filename.extension' and error message\n",
        "  Returns:\n",
        "    True if the photos are similar\n",
        "    False if the photos are different\n",
        "    error_message if some of the paths is invalid\n",
        "  '''\n",
        "  try:\n",
        "    known_image = face_recognition.load_image_file(img1)\n",
        "    unknown_image = face_recognition.load_image_file(img2)\n",
        "  except FileNotFoundError:\n",
        "    return error_message\n",
        "  try:\n",
        "    biden_encoding = face_recognition.face_encodings(known_image)[0]\n",
        "    unknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
        "    results = face_recognition.compare_faces([biden_encoding], unknown_encoding)\n",
        "    return results[0]\n",
        "  except IndexError:\n",
        "    return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4_wWeqaGPTe"
      },
      "source": [
        "def add_simple_noise_opencv(img1, mean_noise=20, std_noise=3, error_message='BadInput', save_or_not=False):\n",
        "  '''\n",
        "  Takes:\n",
        "    an image in format 'filename.jpg'\n",
        "    mean value for gaussian noise\n",
        "    std for gaussian noise\n",
        "    error message in case sth goes wrong:(\n",
        "    save_or_not Boolean variable to write noised picture ir not\n",
        "  Saves an image in local repo with name 'filename_noised.png' if required\n",
        "  Returns:\n",
        "    noised new pic in the form of an array\n",
        "  '''\n",
        "  try:\n",
        "    image = cv2.imread(img1)\n",
        "    mu, sigma = mean_noise, std_noise\n",
        "    random_noise = np.random.normal(mu, sigma, image.shape).astype(np.uint8)\n",
        "    noised_photo = cv2.add(image, random_noise)\n",
        "    noised_photo = cv2.cvtColor(noised_photo, cv2.COLOR_BGR2RGB)\n",
        "    if save_or_not:\n",
        "      new_name = 'your_photo_simply_noised.png' \n",
        "      noised_photo = cv2.cvtColor(noised_photo, cv2.COLOR_BGR2RGB)\n",
        "      cv2.imwrite(new_name, noised_photo)\n",
        "    return noised_photo\n",
        "  except FileNotFoundError:\n",
        "    return error_message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB7dimYMGhSe"
      },
      "source": [
        "def add_blur(img_path, kernel=(3, 3)):\n",
        "  '''Takes image path and kernel\n",
        "  Returns blurred image array\n",
        "  '''\n",
        "  image = cv2.imread(img_path)\n",
        "  blurred_image = cv2.GaussianBlur(image, kernel, 0)\n",
        "  blurred_image = cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB)\n",
        "  return blurred_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBxGvoflSkCd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "795e7957-f7ca-41fe-da85-ce589614e0d5"
      },
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
        "                                                    weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "def create_adversarial_pattern(input_image, input_label):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(input_image)\n",
        "    prediction = pretrained_model(input_image)\n",
        "    loss = loss_object(input_label, prediction)\n",
        "\n",
        "  # Get the gradients of the loss w.r.t to the input image.\n",
        "  gradient = tape.gradient(loss, input_image)\n",
        "  # Get the sign of the gradients to create the perturbation\n",
        "  signed_grad = tf.sign(gradient)\n",
        "  return signed_grad\n",
        "\n",
        "# Helper function to preprocess the image so that it can be inputted in MobileNetV2\n",
        "def preprocess(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = tf.image.resize(image, (224, 224))\n",
        "  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "  image = image[None, ...]\n",
        "  return image\n",
        "\n",
        "# Helper function to extract labels from probability vector\n",
        "def get_imagenet_label(probs):\n",
        "  decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
        "  return decode_predictions(probs, top=1)[0][0]\n",
        "\n",
        "\n",
        "def add_noise_compare_save_best(image_path, indxx=208):\n",
        "  '''\n",
        "  Adds adversarial noise until comparison with initial photo gives False results. Saves the best pic\n",
        "  '''\n",
        "  pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
        "                                                     weights='imagenet')\n",
        "  pretrained_model.trainable = False\n",
        "  # ImageNet labels\n",
        "  decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
        "  image_raw = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_image(image_raw)\n",
        "  image = preprocess(image)\n",
        "  image_probs = pretrained_model.predict(image)\n",
        "  _, image_class, class_confidence = get_imagenet_label(image_probs)\n",
        "  label = tf.one_hot(indxx, image_probs.shape[-1])\n",
        "  label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "  perturbations = create_adversarial_pattern(image, label)\n",
        "  epsilons = np.arange(0.01, 0.50, 0.001)\n",
        "\n",
        "  for i, eps in enumerate(epsilons):\n",
        "    noised_image_path = 'your_noised_photo.jpg'\n",
        "\n",
        "    # smart noise\n",
        "    adv_x = image + eps*perturbations\n",
        "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "    arr = (adv_x[0].numpy()*0.5 + 0.5).astype(np.float64)\n",
        "    mpl.image.imsave(noised_image_path, arr)\n",
        "    if not compare_photos(noised_image_path, image_path):\n",
        "      return round(eps, 5), perturbations[0]\n",
        "\n",
        "    # gaussian noise\n",
        "    arr = add_simple_noise_opencv(noised_image_path) \n",
        "    mpl.image.imsave(noised_image_path, arr)\n",
        "    if not compare_photos(noised_image_path, image_path):\n",
        "      return round(eps, 5), perturbations[0]\n",
        "\n",
        "    # blur\n",
        "    arr = add_blur(noised_image_path)\n",
        "    mpl.image.imsave(noised_image_path, arr)\n",
        "    if not compare_photos(noised_image_path, image_path):\n",
        "      return round(eps, 5), perturbations[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
            "14540800/14536120 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxRIVh2Hwfni"
      },
      "source": [
        "def create_desired_noise(image_path, indxx=42, epsi = 0):\n",
        "  pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
        "                                                     weights='imagenet')\n",
        "  pretrained_model.trainable = False\n",
        "  # ImageNet labels\n",
        "  decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
        "  image_raw = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_image(image_raw)\n",
        "  image = preprocess(image)\n",
        "  image_probs = pretrained_model.predict(image)\n",
        "  _, image_class, class_confidence = get_imagenet_label(image_probs)\n",
        "  label = tf.one_hot(indxx, image_probs.shape[-1])\n",
        "  label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "  perturbations = create_adversarial_pattern(image, label)\n",
        "  eps = epsi\n",
        "\n",
        "  noised_image_path = 'your_desired_noised_photo.jpg'\n",
        "\n",
        "  # smart noise\n",
        "  adv_x = image + eps*perturbations\n",
        "  adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "  arr = (adv_x[0].numpy()*0.5 + 0.5).astype(np.float64)\n",
        "  mpl.image.imsave(noised_image_path, arr)\n",
        "  return round(eps, 5), perturbations[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxKJycmOXMX2"
      },
      "source": [
        "def add_noise_compare_with_second_save(image1_path, image2_path, indxx=42, to_blur=False):\n",
        "  '''\n",
        "  Adds adversarial noise to first until comparison with second photo gives False results. \n",
        "  Saves the best pic\n",
        "  '''\n",
        "  width, height, channels = cv2.imread(image1_path).shape\n",
        "  dsize = (width, height)\n",
        "  pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
        "                                                     weights='imagenet')\n",
        "  pretrained_model.trainable = False\n",
        "  # ImageNet labels\n",
        "  decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
        "  image_raw = tf.io.read_file(image1_path)\n",
        "  image = tf.image.decode_image(image_raw)\n",
        "  image = preprocess(image)\n",
        "  image_probs = pretrained_model.predict(image)\n",
        "  _, image_class, class_confidence = get_imagenet_label(image_probs)\n",
        "  label = tf.one_hot(indxx, image_probs.shape[-1])\n",
        "  label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "  perturbations = create_adversarial_pattern(image, label)\n",
        "  epsilons = np.arange(0.01, 0.50, 0.001)\n",
        "\n",
        "  # image2_raw = = tf.io.read_file(image2_path)\n",
        "  # image2 = tf.image.decode_image(image2_raw)\n",
        "  # image2 = preprocess(image2)\n",
        "\n",
        "  for i, eps in enumerate(epsilons):\n",
        "    noised_image_path = 'your_noised_photo.jpg'\n",
        "\n",
        "    # smart noise\n",
        "    adv_x = image + eps*perturbations\n",
        "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "    arr = (adv_x[0].numpy()*0.5 + 0.5).astype(np.float64)\n",
        "    mpl.image.imsave(noised_image_path, arr)\n",
        "    if not compare_photos(noised_image_path, image2_path): \n",
        "      return round(eps, 5), perturbations[0]\n",
        "\n",
        "    # gaussian noise\n",
        "    arr = add_simple_noise_opencv(noised_image_path) \n",
        "    mpl.image.imsave(noised_image_path, arr)\n",
        "    if not compare_photos(noised_image_path, image2_path):\n",
        "      return round(eps, 5), perturbations[0]\n",
        "\n",
        "    # blur\n",
        "    if to_blur:\n",
        "      arr = add_blur(noised_image_path)\n",
        "      mpl.image.imsave(noised_image_path, arr)\n",
        "      if not compare_photos(noised_image_path, image2_path):\n",
        "        return round(eps, 5), perturbations[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF4x1Ww29_0r"
      },
      "source": [
        "def noir(input_image):\n",
        "  \"\"\"\n",
        "  Takes path to colored photo and returns 2D array of black and white image\n",
        "  \"\"\"\n",
        "  img = cv2.imread(input_image)\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  img_new = np.zeros_like(img)\n",
        "  img_new[:,:,0] = img_gray\n",
        "  img_new[:,:,1] = img_gray\n",
        "  img_new[:,:,2] = img_gray\n",
        "  return img_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf6DeBjHHIkA"
      },
      "source": [
        "def get_the_noise_img(input_image_path,is_save=False):\n",
        "  # Get the input label of the image.\n",
        "  eps, pert = create_desired_noise(input_image_path, epsi=0.5)\n",
        "  noise = eps*(pert.numpy()*0.5 + 0.5)\n",
        "  if is_save:\n",
        "    noised_path = 'noise_img.jpg'\n",
        "    mpl.image.imsave(noised_path, noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuE1aAPtLRpX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56594abe-2223-4a65-9b00-d0210b6a327e"
      },
      "source": [
        "compare_photos('/content/Screen Shot 2020-09-06 at 19.34.45.png', '/content/square-242.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHOlPp26S3jg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e239e482-13a2-4cb4-904f-7967f99dfcda"
      },
      "source": [
        "cv2.imread('/content/Screen Shot 2020-09-06 at 19.34.45.png').shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(504, 504, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z6KTFj2R5wN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "61df871c-d9f0-40a5-ba7f-03286313fd0b"
      },
      "source": [
        "add_noise_compare_with_second_save('/content/Adir.jpg', '/content/Adir.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.125, <tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n",
              " array([[[ 1., -1.,  1.],\n",
              "         [ 1., -1.,  1.],\n",
              "         [-1., -1.,  1.],\n",
              "         ...,\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1., -1.],\n",
              "         [ 1.,  1.,  1.]],\n",
              " \n",
              "        [[ 1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1.,  1.],\n",
              "         ...,\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1.,  1.],\n",
              "         [ 1.,  1.,  1.]],\n",
              " \n",
              "        [[ 1., -1.,  1.],\n",
              "         [ 1., -1., -1.],\n",
              "         [-1., -1.,  1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1.,  1.]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1., -1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1., -1.]],\n",
              " \n",
              "        [[ 1.,  1.,  1.],\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1., -1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [ 1.,  1.,  1.],\n",
              "         [ 1.,  1.,  1.]],\n",
              " \n",
              "        [[ 1.,  1., -1.],\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1., -1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [ 1.,  1.,  1.]]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqNkUo_Grq5u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbb4e3ad-e008-4987-f6be-14786a9a0c9b"
      },
      "source": [
        "compare_photos('/content/your_desired_noised_photo.jpg', '/content/Adir.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBuuiZ30U3-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "15a22acf-c5d7-4d32-bdb9-e134cd3982d6"
      },
      "source": [
        "create_desired_noise('/content/Adir.jpg', epsi=0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5b47fdef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5, <tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n",
              " array([[[ 1., -1.,  1.],\n",
              "         [ 1., -1.,  1.],\n",
              "         [-1., -1.,  1.],\n",
              "         ...,\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1., -1.],\n",
              "         [ 1.,  1.,  1.]],\n",
              " \n",
              "        [[ 1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1.,  1.],\n",
              "         ...,\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1.,  1.],\n",
              "         [ 1.,  1.,  1.]],\n",
              " \n",
              "        [[ 1., -1.,  1.],\n",
              "         [ 1., -1., -1.],\n",
              "         [-1., -1.,  1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1.,  1.]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1., -1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1., -1.]],\n",
              " \n",
              "        [[ 1.,  1.,  1.],\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1., -1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [ 1.,  1.,  1.],\n",
              "         [ 1.,  1.,  1.]],\n",
              " \n",
              "        [[ 1.,  1., -1.],\n",
              "         [ 1.,  1.,  1.],\n",
              "         [-1., -1., -1.],\n",
              "         ...,\n",
              "         [-1., -1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [ 1.,  1.,  1.]]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOv_CvmxYKfA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}